{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I'll try to add some more days to the last retrieved data from some data that I retrieved when building the code few days before the final version of [notebooks/01-collecting_and_saving_tweets.ipynb](http://localhost:8888/notebooks/twitter_analysis_online_grocery_NL/notebooks/01-collecting_and_saving_tweets.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "TodaysDate = time.strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying to add more days to Jumbo and AH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_file_in_folder(folder,str_file, type_file='csv'):\n",
    "    \"\"\" Given a folder and a part of file's name outputs a list of files paths\"\"\"\n",
    "    \n",
    "    list_files_paths = []\n",
    "    for file_path in glob.glob(folder+'*'+str_file+'*.'+type_file):\n",
    "        try:\n",
    "            list_files_paths.append(file_path)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    if len(list_files_paths):\n",
    "        return list_files_paths\n",
    "    else:\n",
    "        return 'No files containing {}'.format(str_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe_info(result):\n",
    "    \"\"\" Create a dataframe with tweet's data with path of the .csv file, min and max create_at date, \n",
    "    number of tweets, and number of columns \"\"\"\n",
    "    \n",
    "    filepath_list = []\n",
    "    min_created_list = []\n",
    "    max_created_list = []\n",
    "    n_tweet_list = []\n",
    "    n_columns = []\n",
    "    \n",
    "    for file in result:\n",
    "        df = pd.read_csv(file)\n",
    "        df['created_at'] = pd.to_datetime(df['created_at'], infer_datetime_format=True)\n",
    "        filepath_list.append(file)\n",
    "        min_created_list.append(min(df['created_at']))\n",
    "        max_created_list.append(max(df['created_at']))\n",
    "        n_tweet_list.append(df.shape[0])\n",
    "        n_columns.append(df.shape[1])\n",
    "        \n",
    "    dict_df = {'file_path':filepath_list,\n",
    "              'min_created_list':min_created_list,\n",
    "              'max_created_list':max_created_list,\n",
    "              'n_tweet_list':n_tweet_list,\n",
    "              'n_columns':n_columns}\n",
    "    \n",
    "    df_new = pd.DataFrame(dict_df)\n",
    "                \n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "folder = \"../data/tweets/\"\n",
    "result = search_file_in_folder(folder, 'JumboSupermarkt')\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Jumbo = create_dataframe_info(result).sort_values(by=['min_created_list','n_tweet_list'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Jumbo.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I concatenate the most recent file with any of the 3 first files displayed in the dataframe I can go back until 3rd March. One tradeoff is that we have 5 columns more in the last version than in the old one. Let's see what we can do about AH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"../data/tweets/\"\n",
    "result = search_file_in_folder(folder, 'albertheijn')\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info_AH.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info_AH = create_dataframe_info(result).sort_values(by=['min_created_list','n_tweet_list'])\n",
    "df_info_AH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding one of the 3 first files we can go back to 30th March.\n",
    "\n",
    "So let's check the columns we need to drop and concatenate the newer and older csv to increase our range from 30th March 2020 until 22nd June 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info_AH.loc[0,'file_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_AH_2020_06_16 = pd.read_csv(df_info_AH.loc[0,'file_path'])\n",
    "# before checking for difference in the columns between old and new data I'll rename handle to screen_name since both are the same\n",
    "df_AH_2020_06_16.rename(columns={'handle':'screen_name'},inplace=True)\n",
    "df_AH_2020_06_16.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_AH_2020_06_16.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sum(df_AH_2020_06_16['created_at'].str.contains('2020-03'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_AH_2020_06_16['created_at'] = pd.to_datetime(df_AH_2020_06_16['created_at'], infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(df_AH_2020_06_16['created_at']),max(df_AH_2020_06_16['created_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_AH_2020_06_22 = pd.read_csv(df_info_AH.loc[10,'file_path'])\n",
    "df_AH_2020_06_22.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_AH_2020_06_22.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_AH_2020_06_22['created_at'] = pd.to_datetime(df_AH_2020_06_22['created_at'], infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_AH_2020_06_22.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_columns = list(set(df_AH_2020_06_16.columns).intersection(set(df_AH_2020_06_22.columns)))\n",
    "common_columns.sort()\n",
    "common_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this agrees with what we expected\n",
    "len(common_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns we will miss by concatenation the older and newer columns are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(set(df_AH_2020_06_22.columns).difference(set(df_AH_2020_06_16.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I'd like to explore this data as well as use to compare all 3 (online) supermarkets.\n",
    "\n",
    "`language` we need to deal with it because it will be important for the sentiment analysis. For now, I'll add this to the older data with NaN and then I'll try to label it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding 'language' column to df_AH_2020_06_16\n",
    "\n",
    "df_AH_2020_06_16['language'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_AH_2020_06_16.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update common_columns\n",
    "\n",
    "common_columns = list(set(df_AH_2020_06_16.columns).intersection(set(df_AH_2020_06_22.columns)))\n",
    "common_columns.sort()\n",
    "common_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(common_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(set(df_AH_2020_06_22.columns).difference(set(df_AH_2020_06_16.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have 15 columns and we can verify that language is also present in the older data dataframe. Let's concatenate both dataframes and try to deal with the `language` problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_AH_2020_06_16.info(null_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_AH_2020_06_16 = df_AH_2020_06_16[df_AH_2020_06_16['created_at'] <= min(df_AH_2020_06_22['created_at'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_AH_2020_06_16.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(df_AH_2020_06_16['created_at']),max(df_AH_2020_06_16['created_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(df_AH_2020_06_22['created_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate dataframes\n",
    "df_AH_concat = pd.concat([df_AH_2020_06_16,df_AH_2020_06_22[common_columns]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminate duplicates based on create_at and text, keep will be setted to 'last' since we know that in the older \n",
    "# data language will be nan and it is better to keep data that is not nan\n",
    "\n",
    "df_AH_concat = df_AH_concat.loc[df_AH_concat.astype(str).drop_duplicates(subset=['created_at','tweet_id','text']).index]\n",
    "\n",
    "# sorting by 'created_at'\n",
    "df_AH_concat.sort_values(by='created_at',inplace = True)\n",
    "\n",
    "# reset index\n",
    "df_AH_concat.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# save in csv\n",
    "\n",
    "df_AH_concat.to_csv(\"../data/processed/AH_concat_16_and_22_June_\"+TodaysDate+\".csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"../data/processed/AH_concat_16_and_22_June_\"+TodaysDate+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.info(null_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inserting language missing data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
