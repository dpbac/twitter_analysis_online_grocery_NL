{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO DO**\n",
    "\n",
    "https://towardsdatascience.com/exploratory-data-analysis-for-natural-language-processing-ff0046ab3571\n",
    "\n",
    "https://marcobonzanini.com/2015/03/09/mining-twitter-data-with-python-part-2/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#Load-Packages\" data-toc-modified-id=\"Load-Packages-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Load Packages</a></span></li><li><span><a href=\"#User-Timeline-Tweet-Data\" data-toc-modified-id=\"User-Timeline-Tweet-Data-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>User Timeline Tweet Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-User-Timeline-Data\" data-toc-modified-id=\"Load-User-Timeline-Data-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Load User Timeline Data</a></span></li><li><span><a href=\"#Pre-processing\" data-toc-modified-id=\"Pre-processing-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Pre-processing</a></span><ul class=\"toc-item\"><li><span><a href=\"#@albertheijn\" data-toc-modified-id=\"@albertheijn-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>@albertheijn</a></span></li><li><span><a href=\"#@JumboSupermarkten\" data-toc-modified-id=\"@JumboSupermarkten-3.2.2\"><span class=\"toc-item-num\">3.2.2&nbsp;&nbsp;</span>@JumboSupermarkten</a></span></li><li><span><a href=\"#@picnic\" data-toc-modified-id=\"@picnic-3.2.3\"><span class=\"toc-item-num\">3.2.3&nbsp;&nbsp;</span>@picnic</a></span></li></ul></li><li><span><a href=\"#Selecting-features\" data-toc-modified-id=\"Selecting-features-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Selecting features</a></span></li><li><span><a href=\"#Concatenating-Data-within-period-March-30th-and-June-24th\" data-toc-modified-id=\"Concatenating-Data-within-period-March-30th-and-June-24th-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Concatenating Data within period March 30th and June 24th</a></span><ul class=\"toc-item\"><li><span><a href=\"#Cleaning-hashtags-and-source-info\" data-toc-modified-id=\"Cleaning-hashtags-and-source-info-3.4.1\"><span class=\"toc-item-num\">3.4.1&nbsp;&nbsp;</span>Cleaning hashtags and source info</a></span></li></ul></li><li><span><a href=\"#Analysing-language-of-all-tweets\" data-toc-modified-id=\"Analysing-language-of-all-tweets-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Analysing language of all tweets</a></span></li><li><span><a href=\"#Cleaning-Tweets\" data-toc-modified-id=\"Cleaning-Tweets-3.6\"><span class=\"toc-item-num\">3.6&nbsp;&nbsp;</span>Cleaning Tweets</a></span></li></ul></li><li><span><a href=\"#To-work-with-Dutch-language\" data-toc-modified-id=\"To-work-with-Dutch-language-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>To work with Dutch language</a></span></li><li><span><a href=\"#EDA\" data-toc-modified-id=\"EDA-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>EDA</a></span><ul class=\"toc-item\"><li><span><a href=\"#Number-of-followers-and-friends\" data-toc-modified-id=\"Number-of-followers-and-friends-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Number of followers and friends</a></span></li><li><span><a href=\"#Languages-of-messages\" data-toc-modified-id=\"Languages-of-messages-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Languages of messages</a></span></li></ul></li><li><span><a href=\"#Analysing-Queries\" data-toc-modified-id=\"Analysing-Queries-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Analysing Queries</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-query-data\" data-toc-modified-id=\"Load-query-data-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Load query data</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO ADAPT AND INSERT SOMEWHERE**\n",
    "\n",
    "attributes are the following:\n",
    "\n",
    "text: the text of the tweet itself\n",
    "\n",
    "created_at: the date of creation\n",
    "\n",
    "favorite_count, retweet_count: the number of favourites and retweets\n",
    "\n",
    "favorited, retweeted: boolean stating whether the authenticated user (you) have favourited or retweeted this tweet\n",
    "\n",
    "lang: acronym for the language (e.g. “en” for english)\n",
    "\n",
    "id: the tweet identifier\n",
    "\n",
    "We can imagine how these data already allow for some interesting analysis: we can check who is most favourited/retweeted, who’s discussing with who, what are the most popular hashtags and so on. Most of the goodness we’re looking for, i.e. the content of a tweet, is anyway embedded in the text, and that’s where we’re starting our analysis.\n",
    "\n",
    "**MY GOAL**\n",
    "\n",
    "1. Cleaning\n",
    "    Clean text without tokenize (at least no using a tokenizer) neither remove stop words.\n",
    "    \n",
    "    Complement my function checking :\n",
    "    \n",
    "    http://localhost:8889/notebooks/Project_Twitter/planing%20twitter%20project.ipynb\n",
    "    \n",
    "    https://marcobonzanini.com/2015/03/09/mining-twitter-data-with-python-part-2/\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "and create 2 columns:\n",
    "\n",
    "sentiment_score: \n",
    "\n",
    "sentiment: neg (<0), pos (>0), neutral (0)\n",
    "\n",
    "In some vizualizations groupby supermarkt, sentiment and then:\n",
    "\n",
    "- check most frequent words : pos and negative\n",
    "- check timeline of the 3 supermarkts pos and neg\n",
    "- Build Word Cloud using mask (tokenize and remove stop words)\n",
    "\n",
    "- tokenize using split() - dividing by space and use the dutch stopwords because \n",
    "\"It must be trained on a large collection of plaintext in the target language before it can be used.\" https://www.nltk.org/api/nltk.tokenize.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The goal here is to perform some EDA and Sentiment Analysis to infer how user of online supermarkets feel towards these supermarkets in the period from March 30th, 2020 and June 24th, 2020. This period does not cover all period considering the 1st corona case in The Netherlands (February 27th) but as seen in the following graph still covers an important period of the crisis.\n",
    "\n",
    "![corona cases from 27022020 until 25062020](../images/graph_corona_cases_nl_270220_250620.jpg)\n",
    "source: https://www.rivm.nl/coronavirus-covid-19/grafieken\n",
    "\n",
    "Our analysis separated in two parts:\n",
    "\n",
    "1. **[User timeline data](#User-Timeline-Tweet-Data)** \n",
    "2. Search data\n",
    "\n",
    "The following actions are taken in order to achieve our goal.\n",
    "\n",
    "1. **[Load data](#Load-User-Timeline-Data)**\n",
    "2. **[Select interesting features for our analysis](#Pre-processing)**\n",
    "3. **[Concatenate data](#Concatenating-Data-within-period-March-30th-and-June-24th)**\n",
    "4. **Perform analysis over languages present in our data and decide how to deal with it**\n",
    "5. **Clean text for sentiment analysis**\n",
    "6. **Create features that measure sentiment**\n",
    "7. **Perform EDA and Sentiment Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Concatenating Data within period March 30th and June 24th\".replace(\" \",\"-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "TodaysDate = time.strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from my evanescence project\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    \"\"\" remove accents\"\"\"\n",
    "    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "\n",
    "def removebrackets(text):\n",
    "    \"\"\" remove brackets \"\"\"\n",
    "    return re.sub('[\\(\\[].*?[\\)\\]]', ' ', text)\n",
    "\n",
    "def remove_special_chars(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-Z\\s]'\n",
    "    return re.sub(pattern, '', text)\n",
    "\n",
    "def clean_text(text):\n",
    "    \n",
    "    cleaned_text = text.apply(lambda x: removebrackets(x))\n",
    "    cleaned_text = cleaned_text.apply(lambda x: remove_accented_chars(x))\n",
    "    cleaned_text = cleaned_text.apply(lambda x: remove_special_chars(x))\n",
    "    # lowercase\n",
    "    cleaned_text = cleaned_text.apply(lambda x: x.lower())\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# my Trump project\n",
    "\n",
    "#HappyEmoticons\n",
    "emoticons_happy = set([\n",
    "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "    '<3'\n",
    "    ])\n",
    "\n",
    "# Sad Emoticons\n",
    "emoticons_sad = set([\n",
    "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';('\n",
    "    ])\n",
    "\n",
    "#Emoji patterns\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "         u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "         u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "         u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "         u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "         u\"\\U00002702-\\U000027B0\"\n",
    "         u\"\\U000024C2-\\U0001F251\"\n",
    "         \"]+\", flags=re.UNICODE)\n",
    "\n",
    "#combine sad and happy emoticons\n",
    "emoticons = emoticons_happy.union(emoticons_sad)\n",
    "\n",
    "# functions for cleaning \n",
    "\n",
    "# Remove all the non ascii values\n",
    "def remove_non_ascii(text):\n",
    "    return unidecode(str(text))#, encoding = \"utf-8\"))\n",
    "\n",
    "# Remove all the https's from the text\n",
    "def strip_links(text):\n",
    "    return re.sub(\"(?P<url>https?://[^\\s]+)\", '', text)\n",
    "\n",
    "# Remove all the retweets and hastags\n",
    "def strip_char(text):\n",
    "    return re.sub(\"[#@].+?(?= |$)\", '', text)\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    soup = BeautifulSoup(unescape(text), 'lxml')\n",
    "    return soup.text\n",
    "\n",
    "def data_clean(df, column):\n",
    "    cleaning_list = [\"\\r\", \"+\", \":\",\"#\",\"$\",\"/\",\"(<br/>)\",\"(<a).*(>).*(</a>)\",\"(&amp)\",\"(&gt)\",\"(&lt)\",\"(\\xa0)\",\"\\n\"]\n",
    "    for item in cleaning_list:\n",
    "        df[column] = df[column].str.replace(item, \"\")\n",
    "        df[column] = df[column].str.replace(\"\\'s\",\"'s\")\n",
    "    return df[column]\n",
    "        #for text in df[column]:\n",
    "\n",
    "# Cleaninf function that removes emoticons, stopwords and punctuations\n",
    "# def clean_tweets(tweet): \n",
    "#     stop_words = set(stopwords.words('english'))\n",
    "#     word_tokens = word_tokenize(tweet)\n",
    "#     #removing mentions\n",
    "#     tweet = re.sub(r':', '', tweet)\n",
    "#     tweet = re.sub(r'‚Ä¶', '', tweet)\n",
    "#     #replace consecutive non-ASCII characters with a space\n",
    "#     tweet = re.sub(r'[^\\x00-\\x7F]+',' ', tweet)\n",
    "#     #remove emojis from tweet\n",
    "#     tweet = emoji_pattern.sub(r'', tweet)\n",
    "#     #filter using NLTK library append it to a string\n",
    "#     filtered_tweet = [w for w in word_tokens if not w in stop_words]\n",
    "#     filtered_tweet = []\n",
    "#     #looping through conditions\n",
    "#     for w in word_tokens:\n",
    "#     #check tokens against stop words , emoticons and punctuations\n",
    "#         if w not in stop_words and w not in emoticons and w not in string.punctuation:\n",
    "#             filtered_tweet.append(w)\n",
    "#     return ' '.join(filtered_tweet)\n",
    "\n",
    "# clean_tweets modified to keep stopwords\n",
    "\n",
    "def clean_tweets(tweet): \n",
    "#     stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(tweet)\n",
    "    #removing mentions\n",
    "    tweet = re.sub(r':', '', tweet)\n",
    "    tweet = re.sub(r'‚Ä¶', '', tweet)\n",
    "    #replace consecutive non-ASCII characters with a space\n",
    "    tweet = re.sub(r'[^\\x00-\\x7F]+',' ', tweet)\n",
    "    #remove emojis from tweet\n",
    "    tweet = emoji_pattern.sub(r'', tweet)\n",
    "    #filter using NLTK library append it to a string\n",
    "    filtered_tweet = [w for w in word_tokens if not w in stop_words]\n",
    "    filtered_tweet = []\n",
    "    #looping through conditions\n",
    "    for w in word_tokens:\n",
    "    #check tokens against stop words , emoticons and punctuations\n",
    "        if w not in emoticons and w not in string.punctuation:\n",
    "            filtered_tweet.append(w)\n",
    "    return ' '.join(filtered_tweet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To work with Dutch language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy # fast NLP\n",
    "# import pandas as pd # dataframes\n",
    "# import langid # language identification (i.e. what language is this?)\n",
    "# from nltk.classify.textcat import TextCat # language identification from NLTK\n",
    "# from matplotlib.pyplot import plot # not as good as ggplot in R :p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python -m spacy download nl_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Spacy document of our tweets\n",
    "# load an English-language Spacy model\n",
    "# nlp = spacy.load(\"en\")\n",
    "\n",
    "# go to https://spacy.io/models/nl\n",
    "\n",
    "nlp_nl = spacy.load('nl_core_news_sm')\n",
    "\n",
    "df_picnin_nl = df_picnic[df_picnic['language']=='nl']\n",
    "\n",
    "# apply the english language model to our tweets\n",
    "doc = nlp_nl(', '.join(df_picnic['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking 5 longest tweets - just testing model\n",
    "\n",
    "sorted(doc, key=len, reverse=True)[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate all dataframes\n",
    "\n",
    "df_tweet_conc = pd.concat([df_picnic,df_JumboSupermarkt, df_albertheijn])\n",
    "df_tweet_conc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet_conc.shape[0] == df_picnic.shape[0]+df_JumboSupermarkt.shape[0]+df_albertheijn.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of followers and friends "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Languages of messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysing Queries\n",
    "\n",
    "## Load query data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
